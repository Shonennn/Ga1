{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Nam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Library\n",
    "# Library\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import feature_extraction, manifold\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter_Pen.txt has been saved.\n",
      "The_Adventures_of_Tom_Sawyer.txt has been saved.\n",
      "Treasure_Island.txt has been saved.\n",
      "The_Wind_in_the_Willows.txt has been saved.\n",
      "The_Secret_Garden.txt has been saved.\n",
      "Pride_and_Prejudice.txt has been saved.\n",
      "The_Adventures_of_Sherlock_Holmes.txt has been saved.\n"
     ]
    }
   ],
   "source": [
    "# Define the URL and filenames for each book\n",
    "books = [\n",
    "    {\"url\": \"https://www.gutenberg.org/files/16/16-0.txt\", \"filename\": \"Peter_Pen.txt\"},\n",
    "    {\"url\": \"https://www.gutenberg.org/files/74/74-0.txt\", \"filename\": \"The_Adventures_of_Tom_Sawyer.txt\"},\n",
    "    {\"url\": \"https://www.gutenberg.org/cache/epub/120/pg120.txt\", \"filename\": \"Treasure_Island.txt\"},\n",
    "    {\"url\": \"https://www.gutenberg.org/files/289/289-0.txt\", \"filename\": \"The_Wind_in_the_Willows.txt\"},\n",
    "    {\"url\": \"https://www.gutenberg.org/cache/epub/17396/pg17396.txt\", \"filename\": \"The_Secret_Garden.txt\"},\n",
    "    {\"url\": \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\", \"filename\": \"Pride_and_Prejudice.txt\"},\n",
    "    {\"url\": \"https://www.gutenberg.org/files/1661/1661-0.txt\", \"filename\": \"The_Adventures_of_Sherlock_Holmes.txt\"},\n",
    "]\n",
    "\n",
    "# Download and save each book\n",
    "for book in books:\n",
    "    url = book[\"url\"]\n",
    "    filename = book[\"filename\"]\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    print(f\"{filename} has been saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice five books have the same genres and semantically the same\n",
    "## Download: https://www.gutenberg.org\n",
    "# Children literature\n",
    "'Peter Pen\" by J. M. Barrie\n",
    "https://www.gutenberg.org/files/16/16-0.txt\n",
    "\n",
    "\"The Adventures of Tom Sawyer\" by Mark Twain\n",
    "https://www.gutenberg.org/files/74/74-0.txt\n",
    "\n",
    "\"The Wind in the Willows\" by Kenneth Grahame\n",
    "https://www.gutenberg.org/files/289/289-0.txt\n",
    "\n",
    "\"The Secret Garden\" by Frances Hodgson Burnett\n",
    "https://www.gutenberg.org/cache/epub/17396/pg17396.txt\n",
    "\n",
    "\"Treasure Island\" by Robert Louis Stevenson\n",
    "https://www.gutenberg.org/cache/epub/120/pg120.txt\n",
    "\n",
    "# Another text book by the different genre\n",
    "# romance fiction\n",
    "\"Pride and Prejudice\" by Jane Austen\n",
    "https://www.gutenberg.org/cache/epub/1342/pg1342.txt\n",
    "\n",
    "# mystery fiction\n",
    "\"The Adventures of Sherlock Holmes\" by Sir Arthur Conan Doyle\n",
    "https://www.gutenberg.org/files/1661/1661-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_digital_book(book_name, num_partitions, size_partition):\n",
    "    \n",
    "    # Download the digital book from the local directory\n",
    "    book_file = open(book_name, \"r\", encoding='utf-8')\n",
    "    book = book_file.read()\n",
    "    book_file.close()\n",
    "\n",
    "    # use 'word_tokenize' function to tokenize the book into words.\n",
    "    # Then, divide digital book into each partitions of the specified size (100 words)\n",
    "    partitions = nltk.word_tokenize(book)\n",
    "    partitions = [partitions[i : i+size_partition] for i in range(0, len(partitions), size_partition)]\n",
    "    \n",
    "    # Check num_partitions is valid\n",
    "    if num_partitions > len(partitions) or num_partitions < 0:\n",
    "        num_partitions = len(partitions)\n",
    "    partitions = partitions[:num_partitions]\n",
    "    \n",
    "    # Create labels\n",
    "    labels = [book_name[:1]]\n",
    "    # Repeat the labels for the number of times that can fit in the partitions\n",
    "    label_list = labels*(num_partitions//len(labels))\n",
    "    # Get the remainder labels that are needed.\n",
    "    label_list += labels[:num_partitions%len(labels)]\n",
    "    \n",
    "    \n",
    "    # Use regular expression to manipulate the text\n",
    "    # and the regular expression r'[^\\w\\s]' is used to remove non-alphanumeric characters from the text.\n",
    "    partitions = [[re.sub(r'[^\\w\\s]', '', word) for word in partition] for partition in partitions]\n",
    "    \n",
    "    # Remove empty strings from the list\n",
    "    partitions = [[word for word in partition if word] for partition in partitions]\n",
    "\n",
    "#added by Nan\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    partitions = [[word for word in partition if word.lower() not in stop_words] for partition in partitions]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    partitions = [[stemmer.stem(word) for word in partition] for partition in partitions]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    partitions = [[lemmatizer.lemmatize(word) for word in partition] for partition in partitions]\n",
    "\n",
    "#added stoped\n",
    "    # Create pandas dataframe to store the text data\n",
    "    data = {'partition': partitions, 'label': label_list}\n",
    "    df = pd.DataFrame(data) \n",
    "\n",
    "    # Serialize dataframe to csv\n",
    "    df.to_csv(book_name + '.csv', index=False)\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0x99 in position 1504: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m df_list \u001b[39m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m i, book_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(book_list):\n\u001b[1;32m---> 14\u001b[0m     partitions \u001b[39m=\u001b[39m sample_digital_book(book_name, \u001b[39m200\u001b[39;49m, \u001b[39m100\u001b[39;49m)\n\u001b[0;32m     15\u001b[0m     temp \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mpartition\u001b[39m\u001b[39m'\u001b[39m: partitions, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m: label_list[i]})\n\u001b[0;32m     16\u001b[0m     df_list\u001b[39m.\u001b[39mappend(temp)\n",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m, in \u001b[0;36msample_digital_book\u001b[1;34m(book_name, num_partitions, size_partition)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample_digital_book\u001b[39m(book_name, num_partitions, size_partition):\n\u001b[0;32m      2\u001b[0m     \n\u001b[0;32m      3\u001b[0m     \u001b[39m# Download the digital book from the local directory\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     book_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(book_name, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     book \u001b[39m=\u001b[39m book_file\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m      6\u001b[0m     book_file\u001b[39m.\u001b[39mclose()\n\u001b[0;32m      8\u001b[0m     \u001b[39m# use 'word_tokenize' function to tokenize the book into words.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[39m# Then, divide digital book into each partitions of the specified size (100 words)\u001b[39;00m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0x99 in position 1504: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "# Testing for multiple books:\n",
    "book_name1 = \"Peter_Pen.txt\"\n",
    "book_name2 = \"The_Adventures_of_Tom_Sawyer.txt\"\n",
    "book_name3 = \"Treasure_Island.txt\"\n",
    "book_name4 = \"The_Wind_in_the_Willows.txt\"\n",
    "book_name5 = \"The_Secret_Garden.txt\"\n",
    "book_name6 = \"Pride_and_Prejudice.txt\"\n",
    "book_name7 = \"The_Adventures_of_Sherlock_Holmes.txt\"\n",
    "book_list = [book_name1, book_name2, book_name3, book_name4, book_name5, book_name6, book_name7]\n",
    "# If book_list > label_list, add \"f\",\"g\",etc.\n",
    "label_list = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\"]\n",
    "df_list = []\n",
    "for i, book_name in enumerate(book_list):\n",
    "    partitions = sample_digital_book(book_name, 200, 100)\n",
    "    temp = pd.DataFrame({'partition': partitions, 'label': label_list[i]})\n",
    "    df_list.append(temp)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "df.to_csv(\"Children_Literature_books_data.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data from the csv file\n",
    "df = pd.read_csv(\"Children_Literature_books_data.csv\")\n",
    "\n",
    "sw_nltk = stopwords.words('english')\n",
    "print(sw_nltk)\n",
    "print(df['partition'].tolist())\n",
    "# Convert the data into a Bag-of-Words representation using CountVectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words=sw_nltk)\n",
    "count_matrix = count_vectorizer.fit_transform(df['partition'].to_list())\n",
    "\n",
    "# Convert the data into a TF-IDF representation using TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=sw_nltk)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['partition'].to_list())\n",
    "\n",
    "#n-gram\n",
    "# Initialize the CountVectorizer object\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "# Fit and transform the data into n-grams\n",
    "ngram_matrix = vectorizer.fit_transform(df['partition'].to_list())\n",
    "\n",
    "\n",
    "# Get the feature names\n",
    "feature_names_bow = count_vectorizer.get_feature_names_out()\n",
    "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "feature_names_ngram = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Store the BOW and TF-IDF representations as separate dataframes\n",
    "df_bow = pd.DataFrame(count_matrix.toarray(), columns=feature_names_bow)\n",
    "print(\"Here is the BOW result:\",df_bow)\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names_tfidf)\n",
    "print(\"Here is the TF-IDF result:\",df_tfidf)\n",
    "df_ngram=pd.DataFrame(ngram_matrix.toarray(),columns=feature_names_ngram)\n",
    "print(\"Here is the n-gram result:\", df_ngram)\n",
    "\n",
    "\n",
    "#Visualization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get word frequency counts from the CountVectorizer object\n",
    "word_freq = count_matrix.toarray().sum(axis=0)\n",
    "\n",
    "# Create a dataframe of words and their frequency\n",
    "word_df = pd.DataFrame({'word': feature_names_bow, 'freq': word_freq})\n",
    "word_df.sort_values('freq', ascending=False, inplace=True)\n",
    "\n",
    "# Plot the top n words\n",
    "n = 20\n",
    "word_df[:n].plot.bar(x='word', y='freq', legend=False, figsize=(10,5))\n",
    "plt.title('Top {} Words by Frequency'.format(n))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b016a5eeb3a62a8413dbf5d99ec3daf473c3b2ee606428d4d35474acd0d1d411"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
